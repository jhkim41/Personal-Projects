{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728777a-357a-4497-8967-97f1a8fcfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4239e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023 여름 이후 gensim 패키지 import 오류 해결용\n",
    "pip install gensim==3.4.0\n",
    "pip install smart_open==1.9.0\n",
    "pip install -U pyopenssl cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4511a9-768b-4bf6-ace9-3d4e8a734828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Made by JongHyun Kim\n",
    "\n",
    "## Base model made on July, 2022\n",
    "## Repaired on Apr, 2023 \n",
    "### Apr 2023(The problem that sentences combined in one str data is recognized as one sentence and not operating properly.--> Added the sentence_tokenizer process in the __init__ function)\n",
    "### Apr 2023(Made the list generated by the LDA contain the index number of topic that has the maximum importance weight)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n",
    "## Added on May 30, 2023 - for Spherical Kmeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from soyclustering import SphericalKMeans\n",
    "import gap_statistic\n",
    "from soyclustering import proportion_keywords\n",
    "\n",
    "## For LLM embedding models\n",
    "import ollama ## LLAMA3\n",
    "\n",
    "OPENAI_APIKEY = \"\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=f\"{OPENAI_APIKEY}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Need to add Autoencoder\n",
    "\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sklearn_stopwords = set(_stop_words.ENGLISH_STOP_WORDS)\n",
    "my_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "## you can tag the 'part of speech' with this to the word\n",
    "def get_pos(w):\n",
    "    tag = pos_tag([w])[0][1][0].upper()\n",
    "    if tag == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "## Stemming based word tokenizer\n",
    "def token_ws(r):\n",
    "    r1 = word_tokenize(r)\n",
    "    r2 = [w.lower() for w in r1 if w.isalpha()]\n",
    "    r3 = [w for w in r2 if not w in my_stopwords]\n",
    "    r4 = [ps.stem(w) for w in r3]\n",
    "    return r4\n",
    "  \n",
    "## Lemmatizing based word tokenizer\n",
    "def token_wl(r):\n",
    "    r1 = word_tokenize(r)\n",
    "    r2 = [w.lower() for w in r1 if w.isalpha()]\n",
    "    r3 = [w for w in r2 if not w in my_stopwords]\n",
    "    r4 = [lem.lemmatize(w, get_pos(w)) for w in r3]\n",
    "    return r4\n",
    "\n",
    "\n",
    "def llama3_embedding(text, model = 'llama3'):\n",
    "    response =ollama.embeddings(model = model, prompt = text)\n",
    "    embedding = response['embedding']\n",
    "    return embedding\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "## Natural language processor\n",
    "class Nlp:\n",
    "    ## You can input texts in the form of list but if you input a str data, it will automatically transform it into the form of list\n",
    "    def __init__(self, texts):\n",
    "        if (type(texts) != list):\n",
    "            try:\n",
    "                test = sent_tokenize(texts)\n",
    "                if len(test) > 1:\n",
    "                    texts = test\n",
    "            except:\n",
    "                texts = [texts]\n",
    "        self.texts = texts      \n",
    "\n",
    "    ## Vectorizer : Choose which word tokenizer you'll use by typing 's' or 'l'. Choose the vectorizing tool by 'count' or 'tfidf'\n",
    "    def Vectorizer(self, tokenizer, vec):\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "        \n",
    "        if vec == 'count':\n",
    "            vectoring = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        elif vec == 'tfidf':\n",
    "            vectoring = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "            \n",
    "        vec_reviews = vectoring.fit_transform(self.texts)\n",
    "        vec_terms = vectoring.get_feature_names_out()\n",
    "        vec_df = pd.DataFrame(vec_reviews.toarray(), columns=vec_terms)\n",
    "        \n",
    "        return vec_df\n",
    "        \n",
    "    ## Cosine similarity. you can input the same things you input in the Vectorizer\n",
    "    ## This shows how similar each text are to each other\n",
    "    def Cos_sim(self, tokenizer, vec):\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "    \n",
    "        if vec == 'count':\n",
    "            vectoring = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        elif vec == 'tfidf':\n",
    "            vectoring = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "            \n",
    "        vec_reviews = vectoring.fit_transform(self.texts)\n",
    "        \n",
    "        ## After going through the preprocessing, this visualize the similarity with Heatmap plot\n",
    "        sim = cosine_similarity(vec_reviews)\n",
    "        plt.figure(figsize=(8,8))\n",
    "        sns.heatmap(sim, vmin = 0, vmax = 1, center = 0, cmap='PiYG', annot = True)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    ## This performs clustering of your text data. you input the tokenizer and the vectorizing tool\n",
    "    def Cluster(self, tokenizer, vec):\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "    \n",
    "        if vec == 'count':\n",
    "            vectoring = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        elif vec == 'tfidf':\n",
    "            vectoring = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "            \n",
    "        vec_reviews = vectoring.fit_transform(self.texts)\n",
    "        vec_terms = vectoring.get_feature_names_out()\n",
    "        vec_df = pd.DataFrame(vec_reviews.toarray(), columns=vec_terms)\n",
    "        \n",
    "        ## Here we go through the 'Elbow method' and will show you the graph of it\n",
    "        distance = []\n",
    "        K = range(1, 10)\n",
    "        for k in K:\n",
    "            km1 = KMeans(n_clusters=k)\n",
    "            km2 = km1.fit(vec_reviews)\n",
    "            d = km2.inertia_\n",
    "            distance.append(d)\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(K, distance)\n",
    "        plt.show()\n",
    "        \n",
    "        ## After showing the graph of 'Elbow method', you should input the number of cluster you will make\n",
    "        num = int(input())\n",
    "        \n",
    "        ## Then this tool will go through Kmeans method and show you the pandas dataframe with a new column 'cluster'\n",
    "        km = KMeans(n_clusters=num, random_state=5)\n",
    "        km.fit(vec_reviews)\n",
    "        group = km.labels_.tolist()\n",
    "        vec_df['cluster'] = group\n",
    "        return vec_df\n",
    "    \n",
    "    def Spherical_Kmeans(self, tokenizer, vec):\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "    \n",
    "        if vec == 'count':\n",
    "            vectoring = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        elif vec == 'tfidf':\n",
    "            vectoring = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "            \n",
    "        vec_reviews = vectoring.fit_transform(self.texts)\n",
    "        vec_terms = vectoring.get_feature_names_out()\n",
    "        vec_array = vec_reviews.toarray()\n",
    "        vec_df = pd.DataFrame(vec_reviews.toarray(), columns=vec_terms)\n",
    "\n",
    "        ## Gap statistics\n",
    "        count_matrix = csr_matrix(vec_array)\n",
    "\n",
    "        vec_float_array = vec_array.astype('float')\n",
    "        optimalK = gap_statistic.OptimalK(n_jobs=-1, parallel_backend='joblib')\n",
    "        num_clusters = optimalK(vec_float_array, cluster_array=np.arange(1, len(vec_float_array)))\n",
    "\n",
    "        ## Spherical K-means based on num_clusters derived from gap statistics\n",
    "        spherical_kmeans = SphericalKMeans(\n",
    "            n_clusters= num_clusters,\n",
    "            max_iter=100,\n",
    "            verbose=1,\n",
    "            init='similar_cut')\n",
    "        labels = spherical_kmeans.fit_predict(count_matrix)\n",
    "\n",
    "        vocabs = [vocab for vocab, idx in sorted(vectoring.vocabulary_.items(), key=lambda x:x[1])]\n",
    "        centers = spherical_kmeans.cluster_centers_\n",
    "\n",
    "        keywords = proportion_keywords(\n",
    "            centers,\n",
    "            labels=labels,\n",
    "            index2word=vocabs)\n",
    "        \n",
    "        group = spherical_kmeans.labels_.tolist()\n",
    "        \n",
    "        vec_df['cluster'] = group\n",
    "\n",
    "        return vec_df, keywords\n",
    "        \n",
    "    ## Sentiment Analysis tool\n",
    "    def SA(self):\n",
    "        try:\n",
    "            sents = sent_tokenize(self.texts)\n",
    "        except:\n",
    "            sents = self.texts\n",
    "            \n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        senti_list = []\n",
    "        for s in sents:\n",
    "            senti = vader.polarity_scores(s)\n",
    "            senti_list.append(senti)\n",
    "            \n",
    "        senti_df = pd.DataFrame(senti_list)\n",
    "        return senti_df\n",
    "        \n",
    "    ## Topic modeling by LDA. Input the tokenizer to use, number of topics you will extract, and the number of words you want to see in each topic\n",
    "    ## for this Topic_LDA, should set two variable (x, y = Variable_Name.Topic_LDA('l', 3, 3)). x will be the list of each Topics, y will be the ratio of topics for each text you input\n",
    "    def Topic_LDA(self, tokenizer, topic_num, word_num):\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "        \n",
    "        text_type = type(self.texts)\n",
    "        if text_type != list:\n",
    "            self.texts = list(self.texts)\n",
    "            \n",
    "        doc_1 = []\n",
    "        for d in self.texts:\n",
    "            d1 = my_tokenizer(d)\n",
    "            doc_1.append(d1)\n",
    "            \n",
    "        gensim_terms = corpora.Dictionary(doc_1)\n",
    "        doc_matrix = [gensim_terms.doc2bow(w) for w in doc_1]\n",
    "        lda = gensim.models.ldamodel.LdaModel\n",
    "        lda_model = lda(doc_matrix, num_topics = topic_num, id2word = gensim_terms, random_state = 0)\n",
    "        topic_LDA_each = [lda_model[d] for d in doc_matrix]\n",
    "\n",
    "        new_test_LDA_topics_each = []\n",
    "        for each in topic_LDA_each:\n",
    "            topic_importance = []\n",
    "            for topics in each:\n",
    "                topic_importance.append(topics[1])\n",
    "            each.append(topic_importance.index(np.max(topic_importance)))\n",
    "            new_test_LDA_topics_each.append(each)\n",
    "\n",
    "        return lda_model.print_topics(num_words = word_num), new_test_LDA_topics_each\n",
    "\n",
    "    def Topic_SVD(self, tokenizer, vec ,component_num):\n",
    "        svd = TruncatedSVD(n_components = component_num , random_state = 0)\n",
    "\n",
    "        my_tokenizer = tokenizer ## Choose a Tokenizer or an embedding model from the 4 options\n",
    "    \n",
    "        if vec == 'count':\n",
    "            vectoring = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        elif vec == 'tfidf':\n",
    "            vectoring = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "\n",
    "        vec_reviews = vectoring.fit_transform(self.texts)\n",
    "        vec_terms = vectoring.get_feature_names_out()\n",
    "        vec_df = pd.DataFrame(vec_reviews.toarray(), columns=vec_terms)\n",
    "\n",
    "        svd.fit_transform(vec_reviews)\n",
    "        svd_topics = svd.components_.argsort()[:,::-1]\n",
    "\n",
    "        svd_singular_values = svd.singular_values_\n",
    "\n",
    "        svd_length = len(svd_topics)\n",
    "        svd_topic_words = []    \n",
    "        for i in range(0,svd_length):\n",
    "            top_words = [vec_terms[x] for x in svd_topics[i, :-1]]\n",
    "            svd_topic_words.append(top_words)\n",
    "\n",
    "        return svd_topic_words, svd_singular_values\n",
    "\n",
    "## BERTopic will be added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfffe60-462a-4784-bfe3-967f18e5ba62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc1 = \"I'm extremely happy and I would reccomend this TV to anyone who is looking for a great TV at an even better price.\"\n",
    "doc2 = \"The price was reasonable and the Roku remote is easy.\"\n",
    "doc3 = \"I give it 5 stars for price and quality. \"\n",
    "doc4 = \"I have no complaints about the image quality.\"\n",
    "doc5 = \"The menus are just so much more smooth and easy to use.\"\n",
    "doc6 = \"The remote is simple and easy to use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769f9f0-ba1d-4362-b4dc-6a27281ca42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = [doc1, doc2, doc3, doc4, doc5, doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = \" \".join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a38db-d2ae-4efe-b914-e7399da3bf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = Nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb66e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test = Nlp(doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e149e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test.Topic_LDA('l', 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec13877-4b17-45b0-ba39-f0583f46cf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.Topic_LDA('l', 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fab9d3-decf-4c0b-b9f2-68cd04b1eb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(review.Vectorizer('l', 'tfidf').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2796521-5a16-404d-bbbd-efee1ae8d9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.Cos_sim('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732acde-79f5-418a-a020-7561ae0a4205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.Cluster('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3912d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_list, svd_df = review.Topic_SVD('l', 'tfidf', 3)\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c3236-56e2-43ee-bf08-cbf05df829b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.SA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4c4f5-e470-498d-931e-4cbdeaa6364e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_2 = 'I am a dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877854cf-d69c-4ee9-8d14-ccc1720f6098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem = Nlp(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09d82a-a63d-42a6-83ab-9098f503f495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245cef60-d788-44e3-9e74-e247dcd7a896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.Vectorizer('l', 'tfidf') ## this will not work since there is only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084a5cb-3e94-4145-a7a3-966f9487c4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.Topic_LDA('l', 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2dda6f-561b-457b-9d23-9465a3f68e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.Cos_sim('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0546a5d-dd78-4a6f-8782-da3653de285d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.Cluster('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f2e42-c403-42a7-8925-b20274877b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esteem.SA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51702286-5d42-4a4c-9241-d0e1c40c184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Hi we are bada.'\n",
    "text2 = 'Today we are going through NLP session.'\n",
    "text3 = 'This is a very challenging course.'\n",
    "text4 = 'Please concentrate and try to understand the structure.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e96fe6-151a-416d-9c8f-88734d34f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text1, text2, text3, text4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1961d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aee9f-435f-41a7-851e-7a5b0672c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = Nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_test = Nlp(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_test.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27be9d9-b5d6-49eb-ba7a-5ee4769499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Vectorizer('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_test.Vectorizer('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d789d-be59-4977-9d82-d9c58fee1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Vectorizer('l', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac37358-db81-4315-87a0-6130cfda9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Cos_sim('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5214c-4abb-4861-9a2f-760f472d64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Cos_sim('ㅣ', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556d4f3-3393-47d1-b905-1b32d94fe267",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Cluster('l', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2af039-5ee8-4193-94c8-8a83973aa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.SA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0638826",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_test.SA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0cd0a-c5a5-4f2d-9415-4ba83412b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.Topic_LDA('l', 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69dfa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_test.Topic_LDA('l', 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eadd5e-5e5a-4982-abef-7719f4952ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
